{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee357489-eb21-40b3-a43d-de812960a3d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "based on:\n",
    "\n",
    "https://cms-opendata-workshop.github.io/workshop2021-lesson-introtrigger/01-introduction/index.html\n",
    "\n",
    "https://twiki.cern.ch/twiki/bin/viewauth/CMS/SWGuideCMSDataAnalysisSchoolCERN2023HWWLongExercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542decd8-a34a-4d58-bfc8-e52beb2d7aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hist\n",
    "import warnings\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Missing cross-reference index \")\n",
    "\n",
    "fname = \"root://xcache//store/mc/RunIISummer20UL17NanoAODv2/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mc2017_realistic_v8-v1/120000/2A2F4EC9-F9BB-DF43-B08D-525B5389937E.root\"\n",
    "events = NanoEventsFactory.from_root(fname, schemaclass=NanoAODSchema, entry_stop=1_000).events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bdb9b-6200-4b69-8692-8df158f494a7",
   "metadata": {},
   "source": [
    "### **Triggers**\n",
    "\n",
    "Collisions at the LHC happen at a rate close to 40 million per second (40 MHz). Once each collision is sensed by the different subdetectors, the amount of information they generate for each one of them corresponds to about what you can fit in a 1 MB file. If we were to record every single collision, it is said (you can do the math) that one can probably fill out all the available disk space in the world in a few days!\n",
    "\n",
    "Fortunately, as you know, not all collisions that happen at the LHC are interesting, so we do not have to record every single one of them. We want to keep the interesting ones and, most importantly, do not miss the discovery-quality ones. In order to achieve that we need a Trigger.\n",
    "\n",
    "Deciding on which events to record is the main purpose of the trigger system. It is like deciding which events to record by taking a quick picture of it and, even though a bit blurry, decide whether it is interesting to keep or not for a future, more thorough inspection.\n",
    "\n",
    "There are hundreds of different triggers in CMS. Each one of them is designed to pick certain types of events, with different intensities and topologies. For instance the `IsoMu27` trigger, will select events with at least one muon with 27 GeV of transverse momentum.\n",
    "\n",
    "In datasets we have flags for the different trigger bits, basically events that do not pass the trigger are weighted 0, those that pass are weighted 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a40f95-8e65-496f-b8c8-14732f39d2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# analysis trigger\n",
    "trigger = \"IsoMu27\"\n",
    "\n",
    "# access high level trigger mask\n",
    "trigger_mask = events.HLT[trigger]\n",
    "trigger_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7096f4d-5d3f-459c-a232-9a13a81315e6",
   "metadata": {},
   "source": [
    "### **MET filters**\n",
    "\n",
    "Similar to the trigger flag, there is a flag that tells whether the event passes a series of filters devised by the JetMET POG to filter anomalous MET events (MC version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3808df7-2c3f-4b10-9caf-ac8a86e225a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "met_filters = [\n",
    "    \"goodVertices\",\n",
    "    \"globalSuperTightHalo2016Filter\",\n",
    "    \"HBHENoiseFilter\",\n",
    "    \"HBHENoiseIsoFilter\",\n",
    "    \"EcalDeadCellTriggerPrimitiveFilter\",\n",
    "    \"BadPFMuonFilter\",\n",
    "    \"BadPFMuonDzFilter\",\n",
    "    \"eeBadScFilter\",\n",
    "    \"ecalBadCalibFilter\"\n",
    "]\n",
    "\n",
    "met_filters_mask = np.ones(len(events), dtype=\"bool\")\n",
    "for mf in met_filters:\n",
    "    if mf in events.Flag.fields:\n",
    "        met_filters_mask = met_filters_mask & events.Flag[mf]\n",
    "        \n",
    "met_filters_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c879e2-9ac0-43d4-8d5f-20565b2adbb1",
   "metadata": {},
   "source": [
    "**manipulating selections**\n",
    "\n",
    "Coffea provide the `PackedSelection` class that can store several boolean arrays in a memory-efficient mannner\n",
    "and evaluate arbitrary combinations of boolean requirements in an CPU-efficient way (supported inputs are 1D numpy or awkward arrays). \n",
    "\n",
    "We can add selections (boolean masks) using the `add()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd366b-fa91-4c01-aa22-03b609507a85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coffea.analysis_tools import PackedSelection\n",
    "\n",
    "# create an instance of PackedSelections\n",
    "selections = PackedSelection()\n",
    "\n",
    "# add trigger mask\n",
    "selections.add(\"trigger\", trigger_mask)\n",
    "\n",
    "# add met filters mask\n",
    "selections.add(\"metfilters\", met_filters_mask)\n",
    "\n",
    "# add leading muon selections\n",
    "selections.add(\"leading_muon_pt\", ak.firsts(events.Muon).pt > 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a160c4-6092-4583-8c12-9cd67f09c9a8",
   "metadata": {},
   "source": [
    "Then, we can access the combination (logic AND operation) of some masks using the `all()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915aa401-6fb8-4e02-8430-d1c47006855c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combining the 'trigger' and 'metfilters'\n",
    "selections.all(\"trigger\", \"metfilters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83371bd-cb4f-46d4-aa2e-2ebe369be789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combining all selections\n",
    "selections.all(\"trigger\", \"metfilters\", \"leading_muon_pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf072a-58e4-47bd-b28a-d159761eace1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Weights and corrections**\n",
    "\n",
    "MC and data have several weights. MC weights are needed first and foremost to normalize the MC sample to the luminosity of the data. Also, event weights are computed to take into account the different scale factors that we use to improve the description of the data.\n",
    "\n",
    "Scale factors (SF) are corrections applied to MC samples to fix imperfections in the simulation. The origin of the mis-modelling could be from the hard scattering (theory uncertainty), or from the simulation of the response of particles with the detector (Geant4), or due to the conditions evolution in time in data (the MC has only one set of conditions), such as noise and radiation damage effects on the detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13760ff0-656c-4290-a041-97d4d80d3a58",
   "metadata": {},
   "source": [
    "We can access certain correction weights directly from the NanoAOD:\n",
    "\n",
    "**Generation weights**\n",
    "\n",
    "In the generation of MC samples, the number of events may be underestimated or overestimated. Hence, these samples come with a weight (known as *genWeight*) that allows us to rectify this underestimation/overestimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7bc303-85bb-4cb3-902d-fb0d4867999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_weight = events.genWeight\n",
    "gen_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddc670-12c3-4226-b23c-6883c1e52825",
   "metadata": {},
   "source": [
    "**L1Prefiring weights**\n",
    "\n",
    "In 2016 and 2017, the gradual timing shift of ECAL was not properly propagated to L1 trigger primitives (TP) resulting in a significant fraction of high $\\eta$ TP being mistakenly associated to the previous bunch crossing. Since Level 1 rules forbid two consecutive bunch crossings to fire, an unpleasant consequence of this (in addition to not finding the TP in the bx 0) is that events can self veto if a significant amount of ECAL energy is found in the region of 2.<|$\\eta$|<3. This effect is not described by the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b91f56-fdaf-4bdf-ac94-d38cf90921ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l1prefiring_weight = events.L1PreFiringWeight.Nom\n",
    "l1prefiring_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747e20f-d903-43d3-8e8d-609b52c38cb0",
   "metadata": {},
   "source": [
    "### **Correctionlib**\n",
    "\n",
    "Other corrections need to be computed using external tools. [Correctionlib](https://github.com/cms-nanoAOD/correctionlib) provide a well-structured JSON data format for a wide variety of ad-hoc correction factors encountered in a typical HEP analysis and a companion evaluation tool suitable for use in C++ and python programs. Here we restrict our definition of correction factors to a class of functions with scalar inputs that produce a scalar output.\n",
    "\n",
    "In python, the function signature is:\n",
    "\n",
    "```\n",
    "def f(*args: str | int | float) -> float:\n",
    "    return ...\n",
    "```\n",
    "\n",
    "All available corrections can be found [here](https://cms-nanoaod-integration.web.cern.ch/commonJSONSFs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96440963-6076-4828-b429-f9f027fc3d9d",
   "metadata": {},
   "source": [
    "**Pileup weights**\n",
    "\n",
    "This weight is needed to equalize the Pile-Up profile in MC to that in Data. You need to understand that most of the time the simulation is done before, or at least partly before, the data taking, thus the PU profile in the MC is a guess of what will happen with data. This weight is the ratio of the PU profile in data to that guess that was used when producing the MC.\n",
    "\n",
    "You can find the description of the pile up function correction [here](https://cms-nanoaod-integration.web.cern.ch/commonJSONSFs/summaries/LUM_2017_UL_puWeights.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efb0c1-903d-407f-a84d-00b5d497b69e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import correctionlib\n",
    "\n",
    "# The SF themselves can be found in the central POG repository\n",
    "pog_path = \"/cvmfs/cms.cern.ch/rsync/cms-nanoAOD/jsonpog-integration\"\n",
    "\n",
    "# path to the pileup correction set (2017 UL MC)\n",
    "pileup_corrections_path = f\"{pog_path}/POG/LUM/2017_UL/puWeights.json.gz\"\n",
    "\n",
    "# define correction set\n",
    "cset = correctionlib.CorrectionSet.from_file(pileup_corrections_path)\n",
    "\n",
    "# get pileup weights\n",
    "pileup_weights = cset[\"Collisions17_UltraLegacy_goldenJSON\"].evaluate(events.Pileup.nPU, \"nominal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e707d-dd9f-47fe-9189-f05dde3a5024",
   "metadata": {},
   "source": [
    "**Manipulating weights**\n",
    "\n",
    "Similar to `PackedSelection`, coffea provide the `Weights` container that keeps track of correction factors and systematic effects that can be encoded as multiplicative modifiers to the event weight (All weights are stored in vector form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36407c-bbc3-4835-8db9-cda4077ed0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coffea.analysis_tools import Weights\n",
    "\n",
    "# create an instance of the Weights container\n",
    "weights_container = Weights(size=len(events), storeIndividual=True)\n",
    "\n",
    "# add nominal weights\n",
    "weights_container.add(\"genweight\", gen_weight)\n",
    "weights_container.add(\"l1prefiring\", l1prefiring_weight)\n",
    "weights_container.add(\"pileup\", pileup_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0b803-90a0-4a92-930a-8c90c5cdbc4d",
   "metadata": {},
   "source": [
    "Then, we can access the combination (multiplication) of all weights using the `weight()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc030fb3-6079-4d84-8978-37ab43a6147a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_container.weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507ed49-41ea-4679-86f9-e23a44b6f8e7",
   "metadata": {},
   "source": [
    "or a some weight(s) using the `partial_weight()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc31a39-5d36-4546-a371-32a12d93a5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_container.partial_weight(include=[\"pileup\", \"l1prefiring\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4c254-b41e-46dc-841f-79ec14d36540",
   "metadata": {},
   "source": [
    "We can access to a statistical summary of the weights using the `weightStatistics` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784cde9-cef5-489a-b033-55a08be5e023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_container.weightStatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d1e44-b876-4ed9-9bc0-38edea02c602",
   "metadata": {},
   "source": [
    "### **Normalize events to luminosity/cross section**\n",
    "\n",
    "The number of expected events is given by\n",
    "\n",
    "$$N = L \\times \\sigma \\times \\epsilon,$$\n",
    "\n",
    "where $L$ is the integrated luminosity, $\\sigma$ is the cross section of the process, and $\\epsilon$ is the selection efficiency $\\frac{N_{after}}{N_{before}}$. \n",
    "\n",
    "Notice that the addition of weights (scale factors) modify the number of events $N_{after}$ and $N_{before}$. These can be compute as the sum of the weights vector before and after selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5480c0b-54e3-4773-8176-e1389f0ea7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute initial number of events (before selections)\n",
    "sumw_before = ak.sum(weights_container.weight()) # N_before\n",
    "\n",
    "# define region selection\n",
    "region_selection = selections.all(\"trigger\", \"metfilters\", \"leading_muon_pt\")\n",
    "\n",
    "# get region weight vector\n",
    "region_weights = weights_container.weight()[region_selection]\n",
    "\n",
    "# compute final number of events (after selections)\n",
    "sumw_after = ak.sum(region_weights) # N_after\n",
    "\n",
    "# compute selection efficiency\n",
    "eff = sumw_after / sumw_before\n",
    "\n",
    "# compute expected number of events\n",
    "L = 41477.88\n",
    "xsec = 365.34\n",
    "L * xsec * eff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d13a7c-cdab-459b-8e35-96ea17a57287",
   "metadata": {
    "tags": []
   },
   "source": [
    "This normalization should also be applied to the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502a7ab-e448-4c9f-bf5f-00a75741770e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# histogram definition\n",
    "muon_pt_axis = hist.axis.Regular(\n",
    "    bins=40, \n",
    "    start=50, \n",
    "    stop=500, \n",
    "    name=\"muon_pt\", \n",
    "    label=\"Leading Muon $p_T$ [GeV]\"\n",
    ")\n",
    "muon_pt_histogram = hist.Hist(\n",
    "    muon_pt_axis, # muon pt axis\n",
    "    hist.storage.Weight(), # weight storage\n",
    ")\n",
    "# histogram filling\n",
    "muon_pt_histogram.fill(\n",
    "    muon_pt=ak.firsts(events.Muon.pt)[region_selection],\n",
    "    weight=region_weights,\n",
    ")\n",
    "# normalize to luminosity/cross section\n",
    "lumi_weight = (L * xsec) / sumw_before\n",
    "muon_pt_histogram = muon_pt_histogram * lumi_weight\n",
    "muon_pt_histogram.plot1d();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f040d0-161f-4b72-8918-c9f154a73aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total number of events\n",
    "ak.sum(muon_pt_histogram.counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef095a30-67a4-4a5d-9abc-23052c1b567c",
   "metadata": {},
   "source": [
    "### **Quiz**: \n",
    "\n",
    "Using the following dataset\n",
    "\n",
    "`root://eospublic.cern.ch//eos/root-eos/HiggsTauTauReduced/DYJetsToLL.root`\n",
    "\n",
    "* Compute the muon ID (`NUM_TightID_DEN_TrackerMuons`), Iso (`NUM_TightRelIso_DEN_TightIDandIPCut`) and TriggerIso (`NUM_IsoMu27_DEN_CutBasedIdTight_and_PFIsoTight`) scale factors. Read the SF from `muon_Z.json.gz'`.\n",
    "\n",
    "\n",
    "    **Hint 1** Corrections only apply to 1D arrays. Therefore, you must flatten (use `ak.flatten()` and `ak.num()`) the arrays to calculate the SF and then unflatten them to the original shape (use `ak.unflatten()`)\n",
    "    \n",
    "    **Hint 2** Note that in some corrections the variables are limited to a defined range (use `np.clip()` so values outside the interval are clipped to the interval edges). \n",
    "    \n",
    "    \n",
    "    **Hint 3:** The event scale factor is obtained by multiplying the scale factors of each object in the event (use `ak.prod()`).\n",
    "    \n",
    "* Compute the number of weighted events before any selection\n",
    "* Select muons:\n",
    "    * Muons with $p_T>35$ GeV\n",
    "    * Muons with $|\\eta|<2.4$\n",
    "    * Muons identified with a tight working point (`tightId`)\n",
    "    * Muons with tight relative isolation (`pfRelIso03_all < 0.15`)\n",
    "* Select events:\n",
    "    * Events triggered by `IsoMu24`\n",
    "    * Events with missing energy (`MET > 50` GeV)\n",
    "* Filter events and compute the number of weighted events after selection\n",
    "* Compute the expected number of events ($L=41477.88$ and $\\sigma=3503.7$)\n",
    "* Define and plot histograms for leading muon's $p_T$, $\\eta$ and $\\phi$ (normalized to lumi/cross section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5cb34-95fe-4bfd-8a3c-1ad080d5ce5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
