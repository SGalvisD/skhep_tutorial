{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on \n",
    "\n",
    "https://hsf-training.github.io/hsf-training-scikit-hep-webpage/\n",
    "\n",
    "https://cms-opendata-workshop.github.io/workshop2022-lesson-ttbarljetsanalysis/01-introduction/\n",
    "\n",
    "https://github.com/iris-hep/analysis-grand-challenge/blob/main/workshops/agctools2022/coffea/coffea_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hist\n",
    "import warnings\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Missing cross-reference index \")\n",
    "\n",
    "def print_ak_array(array):\n",
    "    for i in array:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Physics objects**\n",
    "\n",
    "In the previous lesson we saw there exist schemas for some standard file formats, most prominently *NanoAOD*. The first benefit of instating a schema is a standardization of our fields. There are also other benefits to this structure: as we now have a collection object (`events.muon`), there is a natural place to impose physics methods. \n",
    "\n",
    "[Vector](https://vector.readthedocs.io/en/latest/) is a Python library for 2D, 3D, and Lorentz vectors, especially arrays of vectors, to solve common physics problems in a NumPy-like way.\n",
    "\n",
    "The allowed keyword arguments for 2D vectors are:\n",
    "\n",
    "* `x` and `y` for Cartesian azimuthal coordinates,\n",
    "* `px` and `py` for momentum,\n",
    "* `rho` and `phi` for polar azimuthal coordinates,\n",
    "* `pt` and `phi` for momentum.\n",
    "\n",
    "For 3D vectors, you need the above and:\n",
    "\n",
    "* `z` for the Cartesian longitudinal coordinate,\n",
    "* `pz` for momentum,\n",
    "* `theta` for the spherical polar angle (from 0 to $\\phi$, inclusive),\n",
    "* `eta` for pseudorapidity.\n",
    "\n",
    "For 4D vectors, you need the above and:\n",
    "\n",
    "* `t` for the Cartesian temporal coordinate,\n",
    "* `tau` for the “proper time” (temporal coordinate in the vector’s rest coordinate system),\n",
    "* `E` or `energy` to get four-momentum,\n",
    "* `M` or `mass` to get four-momentum.\n",
    "\n",
    "The `vector.Array` function behaves exactly like the `ak.Array` constructor, except that it makes arrays of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vector\n",
    "\n",
    "# define four vector\n",
    "muon_4vector = vector.Array(\n",
    "    [{\"pt\": 12.8, \"eta\": -0.623, \"phi\": 0.647, \"M\": 0.106}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector properties**\n",
    "\n",
    "Any geometrical coordinate can be computed from vectors in any coordinate system; they’ll be provided or computed as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rho from (rho, phi, eta, t) cylindrical coordinates\n",
    "muon_4vector.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (x, y, z) from (x, y, z, t) cartesian coordinates\n",
    "muon_4vector.x, muon_4vector.y, muon_4vector.z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some properties are not coordinates, but derived from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# muon energy from (pt, eta, phi, E)\n",
    "muon_4vector.E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector methods**\n",
    "\n",
    "Vector methods require arguments (in parentheses), which may be scalars or other vectors, depending on the calculation.\n",
    "\n",
    "For example, in many contexts we will be interested in the angular distances between two objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta\\phi(\\text{obj}_1, \\text{obj}_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu2_4vector = vector.Array(\n",
    "    [{\"pt\": 6.99, \"eta\": 2.08, \"phi\": -0.0908, \"E\": 0.106}]\n",
    ")\n",
    "\n",
    "muon_4vector.deltaphi(mu2_4vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta R (\\text{obj}_1, \\text{obj}_2)= \\sqrt{\\Delta\\phi(\\text{obj}_1, \\text{obj}_2)^2 - \\Delta\\eta(\\text{obj}_1, \\text{obj}_2)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "muon_4vector.deltaR(mu2_4vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NanoAOD schema incorporates this vector behavior in the physical objects. This makes mathematical operations on our muons well-defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file name\n",
    "fname = \"root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root\"\n",
    "events = NanoEventsFactory.from_root(fname, schemaclass=NanoAODSchema, entry_stop=10_000).events()\n",
    "\n",
    "mu1 = ak.firsts(events.Muon)\n",
    "mu2 = ak.firsts(ak.pad_none(events.Muon, 2)[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu1.delta_phi(mu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu1.delta_r(mu2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis: Plotting a $Z$-Peak**\n",
    "\n",
    "With an understanding of basic columnar analysis and our data in the proper format, we are finally ready to turn our attention to our analysis. We want to plot the $Z$-peak. This involves plotting the pair-mass of leptons which have the same flavor (ee or mumu) and opposite sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select good muons\n",
    "good_muons = (\n",
    "    (events.Muon.pt >= 35)\n",
    "    & (np.abs(events.Muon.eta) < 2.4)\n",
    "    & (events.Muon.tightId)\n",
    "    & (events.Muon.pfRelIso04_all < 0.15)\n",
    ")\n",
    "muons = events.Muon[good_muons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we want events with two muons\n",
    "two_muons_cut = ak.num(muons) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want events with opposite charge muons\n",
    "opposite_charge_muons_cut = ak.sum(muons.charge, axis=1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select dimuons \n",
    "dimuons = muons[(two_muons_cut) & (opposite_charge_muons_cut)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the vector behavior of the muons to compute the invariant mass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute invariant mass\n",
    "dimuon_mass = (dimuons[:, 0] + dimuons[:, 1]).mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and fill the dimuon mass histogram\n",
    "dimuon_mass_axis = hist.axis.Regular(\n",
    "    bins=50, start=10, stop=150, name=\"dimuon_mass\", label=\"$m(\\mu, \\mu)$ [GeV]\"\n",
    ")\n",
    "dimuon_mass_histogram = hist.Hist(dimuon_mass_axis)\n",
    "dimuon_mass_histogram.fill(dimuon_mass=dimuon_mass).plot1d()\n",
    "plt.ylabel(\"Events\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 1:** \n",
    "\n",
    "* Select the transverse missing energy and the leading muons and jets\n",
    "* Compute the lepton-Jet-MET total transverse mass\n",
    "\n",
    "$$m_T^{tot}=\\sqrt{(p_T^{\\mu} + p_T^{jet} + p_T^{miss})² - (\\mathbf{p_T^{\\mu}} + \\mathbf{p_T^{jet}} + \\mathbf{p_T^{miss}})²}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Columnar Analysis with Coffea**\n",
    "\n",
    "The traditional way of analyzing data in HEP involves the event loop. In this paradigm, we would write an explicit loop to go through every event (and through every field of an event that we wish to make a cut on). This method of analysis is rather bulky in comparison to the columnar approach, which (ideally) has no explicit loops at all! Instead, the fields of our data are treated as arrays and analysis is done by way of numpy-like array operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/LKcMTG0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Coffea?**\n",
    "\n",
    "Awkward arrays let us access data in a columnar fashion, but that's just the first part of doing an analysis. [Coffea](https://coffeateam.github.io/coffea/) builds upon this foundation with a variety of features that better enable us to do our analyses. These features include:\n",
    "\n",
    "* **Hists** give us ROOT-like histograms. Actually, this is now a standalone package, but it has been heavily influenced by the (old) coffea hist subpackage, and it's a core component of the coffea ecosystem.\n",
    "\n",
    "* **NanoEvents** allows us to apply a schema to our awkward array. This schema imposes behavior that we would not have in a simple awkward array, but which makes our (HEP) lives much easier. On one hand, it can serve to better organize our data by providing a structure for naming, nesting, and cross-referencing fields; on the other, it allows us to add physics object methods (e.g., for LorentzVectors).\n",
    "\n",
    "* **Processors** are coffea's way of encapsulating an analysis in a way that is deployment-neutral. Once you have a Coffea analysis, you can throw it into a processor and use any of a variety of executors (e.g. Dask, Parsl, Spark) to chunk it up and run it across distributed workers. This makes scale-out simple and dynamic for users.\n",
    "\n",
    "* **Lookup tools** are available in Coffea for any corrections that need to be made to physics data. These tools read a variety of correction file formats and turn them into lookup tables.\n",
    "\n",
    "In summary, coffea's features enter the analysis pipeline at every step. They improve the usability of our input (NanoEvents), enable us to map it to a histogram output (Hists), and allow us tools for scaling and deployment (Processors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processors: Deploying at Scale**\n",
    "\n",
    "Now that we have an analysis, we would naturally like to scale it up to a far larger dataset in any practical scenario. More data means more processing time. We try to amend this fact by making use of parallel processing and distributed computing resources, which coffea can be deployed on rather naturally. Importantly, the details of our analysis are entirely independent of our deployment.\n",
    "\n",
    "To expand our analysis, we will use coffea Processors. Processors are coffea’s way of encapsulating an analysis in a way that is deployment-neutral. Once you have a Coffea analysis, you can throw it into a processor and use any of a variety of executors (e.g. Dask, Parsl, Spark) to chunk it up and run it across distributed workers. This makes scale-out simple and dynamic for users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coffea analyses are written in a **Processor** class:\n",
    "\n",
    "* **__init__**: Define an accumulator object (histogram, dictionary, DataFrame or array) that will be fill later. \n",
    "* **process**: Implement the analysis (observables, regions, corrections, etc) and fill the accumulator object.\n",
    "* **postprocess**: Manipulate the accumulator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from coffea import processor\n",
    "\n",
    "class ExampleProcessor(processor.ProcessorABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # define accumulator object \n",
    "        jet_pt_axis = hist.axis.Regular(\n",
    "            bins=50, start=20, stop=1000, name=\"jet_pt\", label=\"Jet $p_T$ [GeV]\"\n",
    "        )\n",
    "        self.output = hist.Hist(jet_pt_axis)\n",
    "        \n",
    "    \n",
    "    def process(self, events):\n",
    "        # create copies of histogram objects\n",
    "        output = copy.deepcopy(self.output)\n",
    "        \n",
    "        # select good jets\n",
    "        jets = events.Jet[events.Jet.pt > 20]\n",
    "\n",
    "        # get leading jet pt\n",
    "        leading_jet_pt = ak.firsts(jets).pt\n",
    "        \n",
    "        # fill output histogram\n",
    "        output.fill(jet_pt=ak.fill_none(leading_jet_pt, 0))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Processor` class gets deployed on an **executor**, which chunks up input data and feeds it in.\n",
    "\n",
    "Currently, two local executors exist: \n",
    "* `iterative_executor`: The iterative executor simply processes each chunk of an input dataset in turn, using the current python thread.\n",
    "* `futures_executor`: It employs python multiprocessing to spawn multiple python processes that process chunks in parallel on the machine\n",
    "\n",
    "Currently, coffea supports four types of distributed executors:\n",
    "\n",
    "* the parsl distributed executor, accessed via `parsl_executor`,\n",
    "\n",
    "* the dask distributed executor, accessed via `dask_executor`,\n",
    "\n",
    "* the Apache Spark distributed executor, accessed via `run_spark_job`,\n",
    "\n",
    "* and the Work Queue Executor distributed executor, accessed via `work_queue_executor`.\n",
    "\n",
    "These executors use their respective underlying libraries to distribute processing tasks over multiple machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define fileset (list of filenames in fileset must be a list or a dict)\n",
    "fileset = {\"dataset\": [fname]}\n",
    "\n",
    "# run a processor using some executor\n",
    "output = processor.run_uproot_job(\n",
    "    fileset,\n",
    "    treename=\"Events\",\n",
    "    processor_instance=ExampleProcessor(),\n",
    "    executor=processor.futures_executor,\n",
    "    executor_args={\n",
    "        \"schema\": processor.NanoAODSchema, \n",
    "        \"workers\": 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output.plot1d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have a pipeline: our input data is chunked, sent off to different workers which each execute the processor on their chunk, and then collected and converged once all workers finish processing.\n",
    "\n",
    "![](https://i.imgur.com/nCxLquc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 2:** \n",
    "* Define a processor class with the $Z$-peak analysis\n",
    "    * Using a `Hist` object as output\n",
    "    * Using a dictionary of Awkward arrays as output\n",
    "* Run the processor using the `iterative_executor` or `futures_execyutor`\n",
    "* Plot the histogram with the dimuon mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
